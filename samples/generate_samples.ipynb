{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data From HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/home/luntaixia/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cnn_ds = load_dataset('cnn_dailymail', '3.0.0', split = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/luntaixia/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-bbb1a9e22bb69e22.arrow\n",
      "Loading cached shuffled indices for dataset at /home/luntaixia/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-6f45b05462e7bd9c.arrow\n"
     ]
    }
   ],
   "source": [
    "samples = (\n",
    "    cnn_ds\n",
    "    .filter(lambda example: (len(example['article']) >= 500) and (len(example['highlights']) >= 20))\n",
    "    .shuffle(seed = 42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/luntaixia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>summ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When she was 15 weeks pregnant with her fifth ...</td>\n",
       "      <td>Clare Van Santen, 37, has Stage 4 breast cance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The 150 people killed in the Germanwings air d...</td>\n",
       "      <td>1,500 people are attending the touching ceremo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sri Lanka's Kumar Sangakkara should reconsider...</td>\n",
       "      <td>Kumar Sangakkara retired from one-day cricket ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The future of Andy Murray’s coaching arrangeme...</td>\n",
       "      <td>Jonas Bjorkman joined Andy Murray's camp on an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two rare paintings used to plan the filming of...</td>\n",
       "      <td>Two rare paintings by art director Jack Martin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11390</th>\n",
       "      <td>Fernando Torres has hailed manager Diego Simeo...</td>\n",
       "      <td>Atletico Madrid host Real Madrid in Champions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11391</th>\n",
       "      <td>There will be a white-hot atmosphere at White ...</td>\n",
       "      <td>Sherwood is a passionate man but he can’t affo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11392</th>\n",
       "      <td>Family members and friends have been left hear...</td>\n",
       "      <td>Madison Small woke up feeling ill on Monday ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11393</th>\n",
       "      <td>Former Redgum frontman John Schumann has slamm...</td>\n",
       "      <td>Footage has emerged of I Was Only 19 being pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11394</th>\n",
       "      <td>An eight-year-old boy was taken to hospital af...</td>\n",
       "      <td>Eight-year-old boy was hospitalised after bein...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11395 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 article  \\\n",
       "0      When she was 15 weeks pregnant with her fifth ...   \n",
       "1      The 150 people killed in the Germanwings air d...   \n",
       "2      Sri Lanka's Kumar Sangakkara should reconsider...   \n",
       "3      The future of Andy Murray’s coaching arrangeme...   \n",
       "4      Two rare paintings used to plan the filming of...   \n",
       "...                                                  ...   \n",
       "11390  Fernando Torres has hailed manager Diego Simeo...   \n",
       "11391  There will be a white-hot atmosphere at White ...   \n",
       "11392  Family members and friends have been left hear...   \n",
       "11393  Former Redgum frontman John Schumann has slamm...   \n",
       "11394  An eight-year-old boy was taken to hospital af...   \n",
       "\n",
       "                                                    summ  \n",
       "0      Clare Van Santen, 37, has Stage 4 breast cance...  \n",
       "1      1,500 people are attending the touching ceremo...  \n",
       "2      Kumar Sangakkara retired from one-day cricket ...  \n",
       "3      Jonas Bjorkman joined Andy Murray's camp on an...  \n",
       "4      Two rare paintings by art director Jack Martin...  \n",
       "...                                                  ...  \n",
       "11390  Atletico Madrid host Real Madrid in Champions ...  \n",
       "11391  Sherwood is a passionate man but he can’t affo...  \n",
       "11392  Madison Small woke up feeling ill on Monday ni...  \n",
       "11393  Footage has emerged of I Was Only 19 being pla...  \n",
       "11394  Eight-year-old boy was hospitalised after bein...  \n",
       "\n",
       "[11395 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    zip(samples['article'], samples['highlights']), \n",
    "    columns = ['article', 'orig_summ']\n",
    ")\n",
    "df['summ'] = df['orig_summ'].apply(\n",
    "    lambda x : nltk.sent_tokenize(x.strip())[0] # get first sentence of the target\n",
    ")\n",
    "df = df.drop(columns=['orig_summ'])\n",
    "df = df[\n",
    "        (df['article'].str.len() < 10000) \n",
    "        & (df['summ'].str.len() < 1000)\n",
    "    ].reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export sample for UI batch scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10).to_csv(\"samples_pairs.csv\")\n",
    "df[['article']].sample(10).to_csv(\"sample_articles_for_batch_prediction.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Fake Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def post_req(url:str, params = None, data: dict = None) -> requests.Response:\n",
    "    headers = {\n",
    "        \"Content-type\": \"application/json\",\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(url, params = params, json=data, headers=headers)\n",
    "    except Exception as e:\n",
    "        print(\"error happen here:\\n\", e)\n",
    "    else:\n",
    "        if r.status_code == 200:\n",
    "            return r\n",
    "        else:\n",
    "            print(\"request code is not 200\")\n",
    "        \n",
    "def get_req(url:str, params = None) -> requests.Response:\n",
    "    headers = {\"Content-type\": \"application/json\"}\n",
    "    try:\n",
    "        r = requests.get(url, params = params, headers=headers)\n",
    "    except Exception as e:\n",
    "        print(\"error happen here:\\n\", e)\n",
    "    else:\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        else:\n",
    "            print(\"request code is not 200\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For local and mlflow backend, they are free of charge, so use whatever you what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mimic_mlflow_batch(samples: pd.DataFrame, send_arize:bool = False):\n",
    "    # get samples from monitoring\n",
    "    # samples = get_req(\n",
    "    #     url = 'http://localhost:9020/sample/pairs', \n",
    "    #     params = dict(num_sample = num_sample)\n",
    "    # )\n",
    "    # get samples from transformer\n",
    "    #samples = df.sample(num_sample).to_dict(orient = 'list')\n",
    "        \n",
    "    # get scores from mlflow model\n",
    "    r = post_req(\n",
    "        url = \"http://localhost:5000/article/summarize_batch\", \n",
    "        data = dict(articles=samples['article'])\n",
    "    )\n",
    "    summs = r.json()\n",
    "    # log to mysql\n",
    "    post_req(\n",
    "        url = 'http://localhost:9020/log/batch', \n",
    "        data = dict(\n",
    "            articles = samples['article'],\n",
    "            summs = summs,\n",
    "            targets = samples['summ'],\n",
    "            model_source = 'Mlflow',\n",
    "            send_arize = send_arize\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mimic_local_batch(samples: pd.DataFrame, send_arize:bool = False):\n",
    "    # get samples from monitoring\n",
    "    # samples = get_req(\n",
    "    #     url = 'http://localhost:9020/sample/pairs', \n",
    "    #     params = dict(num_sample = num_sample)\n",
    "    # )\n",
    "    # get samples from transformer\n",
    "    #samples = df.sample(num_sample).to_dict(orient = 'list')\n",
    "        \n",
    "    # get scores from mlflow model\n",
    "    r = post_req(\n",
    "        url = \"http://localhost:8000/article/summarize_batch\", \n",
    "        data = dict(\n",
    "            articles=dict(articles=samples['article']),\n",
    "            config=dict(num_beans=8, temperature=1.0)    \n",
    "        )\n",
    "    )\n",
    "    summs = r.json()\n",
    "    # log to mysql\n",
    "    post_req(\n",
    "        url = 'http://localhost:9020/log/batch', \n",
    "        data = dict(\n",
    "            articles = samples['article'],\n",
    "            summs = summs,\n",
    "            targets = samples['summ'],\n",
    "            model_source = 'Local',\n",
    "            send_arize = send_arize\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for AWS services (lambda/sagemaker), they are charged so use it less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mimic_lambda_batch(samples: pd.DataFrame, send_arize:bool = False):\n",
    "    # get samples from monitoring\n",
    "    # samples = get_req(\n",
    "    #     url = 'http://localhost:9020/sample/pairs', \n",
    "    #     params = dict(num_sample = num_sample)\n",
    "    # )\n",
    "    # get samples from transformer\n",
    "    #samples = df.sample(num_sample).to_dict(orient = 'list')\n",
    "        \n",
    "    # get scores from mlflow model\n",
    "    r = post_req(\n",
    "        url = \"xxxxxx\", # lambda api gateway endpoint here \n",
    "        data = dict(articles=samples['article'], num_beans=8, temperature=1.0)\n",
    "    )\n",
    "    summs = r.json()\n",
    "    # log to mysql\n",
    "    post_req(\n",
    "        url = 'http://localhost:9020/log/batch', \n",
    "        data = dict(\n",
    "            articles = samples['article'],\n",
    "            summs = summs,\n",
    "            targets = samples['summ'],\n",
    "            model_source = 'Lambda',\n",
    "            send_arize = send_arize\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import random\n",
    "\n",
    "loop = tqdm(random.choices(\n",
    "    population=[mimic_mlflow_batch, mimic_local_batch, mimic_lambda_batch],\n",
    "    weights = [0.5, 0.49, 0.01],\n",
    "    # population=[mimic_mlflow_batch, mimic_local_batch],\n",
    "    # weights = [0.7, 0.3],\n",
    "    k = 500\n",
    "))\n",
    "for func in loop:\n",
    "    n = random.randint(2, 5)\n",
    "    loop.set_description(f\"{func.__name__}[{n}]\")\n",
    "    \n",
    "    df_sample = df.sample(n).to_dict(orient = 'list')\n",
    "    try:\n",
    "        func(samples=df_sample)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### change time stamp randomly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we assume on average, each query will arive on a 6min (360s) interval (i.e., there will be 10 query in one hour on average), then the query interval will follow exponential distribution, wthl scale factor equal to 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_dt = datetime(2023, 6, 9 )\n",
    "num_samples = 4800 # make sure this number is larger than the actual sample size\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "intervals = rng.exponential(scale=360, size=num_samples).round(0)\n",
    "cum_intervals = intervals.cumsum()\n",
    "dts = [start_dt]\n",
    "for i in range(num_samples):\n",
    "    dts.append(start_dt + timedelta(seconds=cum_intervals[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly assign the generated dates to the samples\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  password=\"luntaixia\",\n",
    "  database=\"summarizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mydb.cursor() as cursor:\n",
    "    sql = f\"\"\"\n",
    "    select prediction_id \n",
    "    from summarizer.summarize_log\n",
    "    \"\"\"\n",
    "    cursor.execute(sql)\n",
    "    r = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d643de475e744f8b0163a2dac708fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4661 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "dts_ = iter(dts)\n",
    "with mydb.cursor() as cursor:\n",
    "    for id_ in tqdm(r):\n",
    "        \n",
    "        sql = f\"\"\"\n",
    "        update summarizer.summarize_log\n",
    "        set prediction_ts = '{next(dts_).strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "        where prediction_id = '{id_[0]}'\n",
    "        \"\"\"\n",
    "        cursor.execute(sql)\n",
    "mydb.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
